{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Prediction from name, using Deep Learning\n",
    "\n",
    "Deep Neural Networks can be used to extract features in the input and derive higher level abstractions. This technique is used regularly in vision, speech and text analysis. In this exercise, we build a deep learning model that would identify low level features in texts containing people's names, and would be able to classify them in one of two categories - Male or Female.\n",
    "\n",
    "## Recurrent Neural Networks and Long Short Term Memory\n",
    "Since we have to process sequence of characters, Recurrent Neural Netwrosk are a good fit for this problem. Whenever we have to persist learning from data previously seen, traditional Neural Networks fail. Recurrent Neural Networks contains loops in the graph, that allows them to persist data in memory. Effective the loops facilitate passing multiple copies of information to be passed on to next step.\n",
    "<details>\n",
    "<summary><strong>Recurrent Neural Network - Loops (expand to view diagram)</strong></summary><p>\n",
    "    ![Recurrent Neural Network - Loops](images/RNN-unrolled.png \"Recurrent Neural Network - Loops\")\n",
    "</p></details>\n",
    "\n",
    "\n",
    "In practice however, when we need to selectively memorize or forget patterns seen in the past, based on the context, plain vanilla RNNs do not seem to perform so well. Instead we can use a special type of RNN, that can retain information in long term, and thus works better in understanding the contextual relation between patterns observed. They are known as Long Short Term memory.\n",
    "\n",
    "The nodes in an LSTM networks consusts of remember/forget gates to retain or pass patterns learnt in sequence useful for predicting target variable. These gates are a way to optionally let information through and tends to the ability of LSTM networks to remove or add information to the cell state in regulated manner.\n",
    "<details>\n",
    "<summary><strong>LSTM - Chains (expand to view diagram)</strong></summary><p>\n",
    "    ![LSTM - Chains](images/LSTM3-chain.png \"LSTM - Chains\")\n",
    "</p></details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "The problem we are trying to solve is to predict whether a given name belongs to a male or female. We will use supervised learning, where the character sequence making up the names would be `X` variable, and the flag indicating **Male(M)** or **Female(F)**  wuold be `Y` variable.\n",
    "\n",
    "We use a stacked 2-Layer LSTM model and a final dense layer with softmax activation as our network architecture. We use categorical cross-entropy as loss function, with an adam optimizer. We also add a 20% dropout layer is added for regularization to avoid over-fitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "*  We will use Keras deep learning library to build the network. THerefore we import the symbolic interfaces needed.\n",
    "* We also use pandas data frames to load and slice-and-dice data\n",
    "* Finally we need numpy for matric manipulation    \n",
    "* While running on SageMaker Notebook Instance, we choose conda_tensorflow kernel, so that Keras code is compiled to use tensorflow in the backend. \n",
    "* If you choose P2 and P3 class of instances for your Notebook, using Tensorflow ensures the low level code takes advantage of all available GPUs. So further dependencies needs to be installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "ebc23d43-698a-4a51-8bf6-791bc3d425c2"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.models import load_model\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import time\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "* Training data that we will be using to train the LSTM model is derived from US Government's SSA records of baby names registered. \n",
    "* Original dataset is split into separate text files for names registered every year, starting from 1880.\n",
    "Each record in each year's files contain the name, the gender identifier, and a count showing how many of those names have been registered.\n",
    "* First step in data preparation is to concatenate data in all year specific files into a single file.\n",
    "* We then load the raw data into memory into a Pandas dataframe and get rid of the temporary downloaded folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir download ;  cd download ; wget https://www.ssa.gov/oact/babynames/names.zip ; unzip -qq names.zip \n",
    "! cat download/yob*.txt > download/allnames.txt\n",
    "filename = 'download/allnames.txt'\n",
    "data=pd.read_csv(filename, sep=',', names = [\"Name\", \"Gender\", \"Count\"])\n",
    "!rm -rf download\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleanup\n",
    "Since the training data contains duplicates and names that have been used to represent both Male and Female genders, we'll remediate the solution using following approach:\n",
    "* Order the names by Name and Gender\n",
    "* Add the count for each group of unique Name-Gender combination\n",
    "* Iterate through the unique groups, and where a name is used for both Male and Female, choose to retain th entry with higher count\n",
    "* Create a new clean data frame containing only unique records mapping each name to a single gender\n",
    "* Create a dictionary that will have the Name as keys and gender (with higher sum count) as values.\n",
    "* We loop through the indexes of the grouped data frame and populate the entries into this dictionary following the logic as described above.\n",
    "* After the dictionary is populated, we create a clean data frame using the keys and values as coulmns.\n",
    "* Shuffle the data and save the clean data into a file, which we'll also use in subsequent phases of model training.\n",
    "* Finally save the data into an S3 bucket of your choice (If you were to orchestrate a pipeline to train, deploy and host the model, the container you create will need access to data on an S3 bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "grouped_data = data.groupby( [ \"Name\", \"Gender\"] ).apply(lambda x: x.Count.sum()).to_frame()\n",
    "grouped_data.columns = ['Count']\n",
    "names={}\n",
    "\n",
    "max_count = len(grouped_data.index.values)\n",
    "\n",
    "for i in tqdm(range(max_count), unit=\" records\"):\n",
    "    if i > 0 and grouped_data.index[i][0] == grouped_data.index[i-1][0]:\n",
    "        if grouped_data.values[i][0] > grouped_data.values[i-1][0]:\n",
    "            names[grouped_data.index[i][0]] = grouped_data.index[i][1]\n",
    "        else:\n",
    "            names[grouped_data.index[i][0]] = grouped_data.index[i-1][1]\n",
    "    else:\n",
    "        names[grouped_data.index[i][0]] = grouped_data.index[i][1]\n",
    "        \n",
    "clean_data = pd.DataFrame(list(names.items()), columns=['Name', 'Gender']).sample(frac=1).reset_index(drop=True)        \n",
    "clean_data = clean_data.sample(frac=1).reset_index(drop=True)\n",
    "print(clean_data.shape)\n",
    "print(clean_data.loc[clean_data['Name'] == 'Mary'])\n",
    "print(clean_data.loc[clean_data['Name'] == 'John'])\n",
    "!mkdir -p ../data\n",
    "clean_data.to_csv('../data/name-gender.txt',index=False,header=False)\n",
    "s3bucketname = input(\"Enter the Name of your S3 bucket : \")\n",
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file('../data/name-gender.txt', s3bucketname, 'data/name-gender.txt')\n",
    "s3.ObjectAcl(s3bucketname,'data/name-gender.txt').put(ACL='public-read')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature representation\n",
    "Before we start buiding the model, we need to represent the data in a format that we can feed into the LSTM model that we'll be creating using the following steps.\n",
    "\n",
    "* Although we already have the cleaned data loaded as a data frame, we re-load the data fresh from the S3 location. That way we'll know for sure that our cleaned data is of good quality.\n",
    "* We need to convert the names into numeric arrays, using one-hot encoding scheme. The length of the arrays representing the names need to be as long as the longest name record we have. Therefore we check for the longest name length and have it in a variable.\n",
    "* Next we derive the set of alphabets used in all the names together.\n",
    "* In order for one-hot encoding to work, we need to assign index values to each of these characters. Since all characters in english alphabet are used in names, naturally we use sequential values as indices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f72fa157-ccea-4320-8b31-82e7a03a5f1d"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"https://s3.amazonaws.com/{}/data/name-gender.txt\".format(s3bucketname)\n",
    "training_data=pd.read_csv(filename, sep=',', names = [\"Name\", \"Gender\"])\n",
    "training_data = shuffle(training_data)\n",
    "\n",
    "#number of names\n",
    "num_names = training_data.shape[0]\n",
    "print(\"Number of names: {}\".format(num_names))\n",
    "\n",
    "# length of longest name\n",
    "max_name_length = (training_data['Name'].map(len).max())\n",
    "print(\"Maximum length of a name: {}\".format(max_name_length))\n",
    "\n",
    "#Concatenate all names into one long string, case insensitive\n",
    "names = training_data['Name'].values\n",
    "txt = \"\"\n",
    "for n in names:\n",
    "    txt += n.lower()\n",
    "\n",
    "#Created a sorted set of alphabets where each alphabet appears only once\n",
    "chars = sorted(set(txt))\n",
    "alphabet_size = len(chars)\n",
    "print('Alphabet size: {}'.format(alphabet_size))\n",
    "print(\"Alphabets used : {}\".format(chars))\n",
    "\n",
    "#Assign indices to characters\n",
    "char_indices = dict((str(chr(c)), i) for i, c in enumerate(range(97,123)))\n",
    "alphabet_size = 123-97\n",
    "char_indices['max_name_length'] = max_name_length\n",
    "print(\"Character indices: \", char_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoded array would be of dimension `n` `*` `m` `*` `a`, where :\n",
    "* `n` = Number of name records, \n",
    "* `m` = Maximum length of a record, and \n",
    "* `a` = Size of alphabet\n",
    "\n",
    "Each of the `n` name records would be represented by 2-dimensional matrix of fixed size. This matrix would have number of rows equal to the maximum length of a name record. Each row would be of size equal to the alphabet size.<p>\n",
    "For each position of a character in a given name, a row of this 2-dimensinal matrix would be either all zeroes (if no alphabets present in the corresponding position), or a row vector with a `1` in the position of the alphabet indicated in the index (and zeroes in other positions). \n",
    "    \n",
    "So, the name `Mary` would look like, followed of course by 11 rows of all zeroes (because length of all encoded names has to be 15) <p>\n",
    "m => [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]<br>\n",
    "a => [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]<br>\n",
    "r => [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]<br>\n",
    "y => [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]    \n",
    "    \n",
    "* We begin encoding `X` variable by taking a tensor containing all zeroes. Observe the dimensions matches the above description.\n",
    "* Then we iterate through each character in each name records and selective turn the matching elements (as in the character index) to ones.\n",
    "* We encode the `Y` variable by simply creating a column vector with zeroes representing Female and ones represnting Male, and check to ensure that dimensions of `X` and `Y` are compatible.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((num_names, max_name_length, alphabet_size))\n",
    "print(X.shape)\n",
    "for i,name in enumerate(names):\n",
    "    name = name.lower()\n",
    "    for t, char in enumerate(name):\n",
    "        X[i, t,char_indices[char]] = 1\n",
    "\n",
    "np.set_printoptions(linewidth=200)\n",
    "print(X[np.where(names=='Mary'),:,:])\n",
    "\n",
    "Y = np.ones((num_names,2))\n",
    "Y[training_data['Gender'] == 'F',0] = 0\n",
    "Y[training_data['Gender'] == 'M',1] = 0\n",
    "print(Y.shape)\n",
    "data_dim = alphabet_size\n",
    "timesteps = max_name_length\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "We build a stacked LSTM network with a final dense layer with softmax activation (many-to-one setup).<p>\n",
    "Categorical cross-entropy loss is used with adam optimizer.<p>\n",
    "A 20% dropout layer is added for regularization to avoid over-fitting.<p>\n",
    "We train this model for 10 epochs, with a batch size of 64. Too large a batch size may result in out of memory error.<p>\n",
    "During training we designate 20% of training data (randomly chosen) to be used as validation data. Validation is never presented to the model during training, instead used to ensure that the model works well with data that it has never seen.<p>\n",
    "This confirms we are not over-fitting, that is the model is not simply memoriziing the dat it sees, and that it can generalize it's learning.<p>\n",
    "After training just for one epoch, you should see about 80% of accuracy.<p>\n",
    "With `t2.medium` instance training one epoch takes about 15 minutes of time (11 ms/step). Training speed dramatically increases if you use a `p2` or `p3` type instances.<p>\n",
    "Using a `p3.16xlarge` instance type for example, we can easily run 20 epochs of training on this model in about 5 minutes of time (430 µs/step) and achieve above 90% of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "34f06cf2-7beb-4fb8-9627-e9e029a17256"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(timesteps, data_dim)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, Y, validation_split=0.10, epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing\n",
    "To test the accuracy of the model, we need same pre-processing as we did on training data (one-hot encoding using the same character indices)<p>\n",
    "We feed this one-hot encoded test data to the model, and the `predict` generates a vector, similar to the training labels vector we used before. Except in this case, it contains what model thinks the gender represnted by each of the test records.<p>\n",
    "To present data intutitively, we simply map it back to `Male` / `Female`, from the `0` / `1` flag.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = input(\"Enter name : \")\n",
    "val_no_special = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', test_name)\n",
    "val_space_collapse = re.sub( '\\s+', ' ', val_no_special).strip()\n",
    "names_test = word_tokenize(val_space_collapse)\n",
    "num_test = len(names_test)\n",
    "\n",
    "X_test = np.zeros((num_test, max_name_length, alphabet_size))\n",
    "\n",
    "for i,name in enumerate(names_test):\n",
    "    name = name.lower()\n",
    "    for t, char in enumerate(name):\n",
    "        X_test[i, t,char_indices[char]] = 1\n",
    "        \n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "for i,name in enumerate(names_test):\n",
    "    print(\"{} is {}\".format(names_test[i],\"Male\" if predictions[i][0]>predictions[i][1] else \"Female\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving\n",
    "Our job is done, we satisfied ourselves that the scheme works, and that we have a somewhat useful model that we can use to predict the gender of people from their names.<p>\n",
    "In order to orchestrate the ML pipeline however, we need to save the model file (containing the weights), and the character indices (including the length of maximum name) to an S3 location.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../model\n",
    "model.save('../model/lstm-gender-classifier-model.h5')\n",
    "np.save('../model/lstm-gender-classifier-indices.npy', char_indices) \n",
    "!tar -zcvf ../model.tar.gz ../model\n",
    "s3.meta.client.upload_file('../model.tar.gz', s3bucketname, 'model/model.tar.gz')\n",
    "!rm -rf ../model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model hosting\n",
    "\n",
    "Amazon SageMaker provides a powerful orchestration framework that you can use to productionize any of your own machine learning algorithm, using any machine learning framework and programming languages.<p>\n",
    "This is possible because SageMaker, as a manager of containers, have standarized ways of interacting with your code running inside a Docker container. Since you are free to build a docker container using whatever code and depndency you like, this gives you freedom to bring your own machinery.<p>\n",
    "In the following steps, we'll containerize the prediction code and host the model behind an API endpoint.<p>\n",
    "This would allow us to use the model from web-application, and put it into real use.<p>\n",
    "The boilerplate code, which we affectionately call the `Dockerizer` framework, was made available on this Notebook instance by the Lifecycle Configuration that you used. Just look into the folder and ensure the necessary files are available as shown.<p>\n",
    "    \n",
    "    <home>    \n",
    "    |\n",
    "    ├── container\n",
    "        │\n",
    "        ├── byoa\n",
    "        |   |\n",
    "        │   ├── train\n",
    "        |   |\n",
    "        │   ├── predictor.py\n",
    "        |   |\n",
    "        │   ├── serve\n",
    "        |   |\n",
    "        │   ├── nginx.conf\n",
    "        |   |\n",
    "        │   └── wsgi.py\n",
    "        |\n",
    "        ├── build_and_push.sh\n",
    "        │   \n",
    "        ├── Dockerfile.cpu\n",
    "        │        \n",
    "        └── Dockerfile.gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../container')\n",
    "os.getcwd()\n",
    "!ls -Rl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Dockerfile` describes the container image and the accompanying script `build_and_push.sh` does the heavy lifting of building the container, and uploading it into an Amazon ECR repository\n",
    "* Sagemaker containers that we'll be building serves prediction request using a Flask based application. `wsgi.py` is a wrapper to invoke the Flask application, while `nginx.conf` is the configuration for the nginx front end and `serve` is the program that launches the gunicorn server. These files can be used as-is, and are required to build the webserver stack serving prediction requests, following the architecture as shown:\n",
    "![Request serving stack](images/stack.png \"Request serving stack\")\n",
    "\n",
    "* The file named `predictor.py` is where we need to package the code for generating inference using the trained model that was saved into an S3 bucket location by the training code during the training job run.<p>\n",
    "* We'll write code into this file using Jupyter magic command - `writefile`.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile byoa/predictor.py\n",
    "# This is the file that implements a flask server to do inferences. It's the file that you will modify to\n",
    "# implement the scoring for your own algorithm.\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from io import StringIO\n",
    "import sys\n",
    "import signal\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.models import load_model\n",
    "import flask\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from os import listdir, sep\n",
    "from os.path import abspath, basename, isdir\n",
    "from sys import argv\n",
    "\n",
    "prefix = '/opt/ml/model'\n",
    "model_path = os.path.join(prefix, 'model')\n",
    "\n",
    "# A singleton for holding the model. This simply loads the model and holds it.\n",
    "# It has a predict function that does a prediction based on the model and the input data.\n",
    "\n",
    "class ScoringService(object):\n",
    "    model_type = None           # Where we keep the model type, qualified by hyperparameters used during training\n",
    "    model = None                # Where we keep the model when it's loaded\n",
    "    graph = None\n",
    "    indices = None              # Where we keep the indices of Alphabet when it's loaded\n",
    "    \n",
    "    @classmethod\n",
    "    def get_indices(cls):\n",
    "        #Get the indices for Alphabet for this instance, loading it if it's not already loaded        \n",
    "        if cls.indices == None:\n",
    "            model_type='lstm-gender-classifier'\n",
    "            index_path = os.path.join(model_path, '{}-indices.npy'.format(model_type))\n",
    "            if os.path.exists(index_path):\n",
    "                cls.indices = np.load(index_path).item()\n",
    "            else:\n",
    "                print(\"Character Indices not found.\")\n",
    "        return cls.indices\n",
    "\n",
    "    @classmethod\n",
    "    def get_model(cls):\n",
    "        #Get the model object for this instance, loading it if it's not already loaded  \n",
    "        if cls.model == None:\n",
    "            model_type='lstm-gender-classifier'\n",
    "            mod_path = os.path.join(model_path, '{}-model.h5'.format(model_type))\n",
    "            if os.path.exists(mod_path):\n",
    "                cls.model = load_model(mod_path)\n",
    "                cls.model._make_predict_function()\n",
    "                cls.graph = tf.get_default_graph()\n",
    "            else:\n",
    "                print(\"LSTM Model not found.\")\n",
    "        return cls.model\n",
    "    \n",
    "    @classmethod\n",
    "    def predict(cls, input):\n",
    "\n",
    "        mod = cls.get_model()\n",
    "        ind = cls.get_indices()\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        if mod == None:\n",
    "            print(\"Model not loaded.\")\n",
    "        else:\n",
    "            if 'max_name_length' not in ind:\n",
    "                max_name_length = 15\n",
    "                alphabet_size = 26\n",
    "            else:\n",
    "                max_name_length = ind['max_name_length']\n",
    "                ind.pop('max_name_length', None)\n",
    "                alphabet_size = len(ind)\n",
    "\n",
    "            inputs_list = input.strip('\\n').split(\",\")\n",
    "            num_inputs = len(inputs_list)\n",
    "\n",
    "            X_test = np.zeros((num_inputs, max_name_length, alphabet_size))\n",
    "\n",
    "            for i,name in enumerate(inputs_list):\n",
    "                name = name.lower().strip('\\n')\n",
    "                for t, char in enumerate(name):\n",
    "                    if char in ind:\n",
    "                        X_test[i, t,ind[char]] = 1\n",
    "\n",
    "            with cls.graph.as_default():\n",
    "                predictions = mod.predict(X_test)\n",
    "\n",
    "            for i,name in enumerate(inputs_list):\n",
    "                result[name] = 'M' if predictions[i][0]>predictions[i][1] else 'F'\n",
    "                print(\"{} ({})\".format(inputs_list[i],\"M\" if predictions[i][0]>predictions[i][1] else \"F\"))\n",
    "\n",
    "        return json.dumps(result)\n",
    "    \n",
    "# The flask app for serving predictions\n",
    "app = flask.Flask(__name__)\n",
    "\n",
    "@app.route('/ping', methods=['GET'])\n",
    "def ping():\n",
    "    #Determine if the container is working and healthy.\n",
    "    # Declare it healthy if we can load the model successfully.\n",
    "    model = ScoringService.get_model()\n",
    "    indices = ScoringService.get_indices()\n",
    "    health = model is not None and indices is not None\n",
    "    status = 200 if health else 404\n",
    "    return flask.Response(response='\\n', status=status, mimetype='application/json')\n",
    "\n",
    "@app.route('/invocations', methods=['POST'])\n",
    "def transformation():\n",
    "    #Do an inference on a single batch of data\n",
    "    data = None\n",
    "\n",
    "    # Convert from CSV to pandas\n",
    "    if flask.request.content_type == 'text/csv':\n",
    "        data = flask.request.data.decode('utf-8')\n",
    "    else:\n",
    "        return flask.Response(response='This predictor only supports CSV data', status=415, mimetype='text/plain')\n",
    "\n",
    "    print('Invoked with {} records'.format(data.count(\",\")+1))\n",
    "\n",
    "    # Do the prediction\n",
    "    predictions = ScoringService.predict(data)\n",
    "\n",
    "    result = \"\"\n",
    "    for prediction in predictions:\n",
    "        result = result + prediction\n",
    "\n",
    "    return flask.Response(response=result, status=200, mimetype='text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Container publishing\n",
    "\n",
    "In order to host and deploy the trained model using SageMaker, we need to build the `Docker` containers, publish it to `Amazon ECR` repository, and then either use SageMaker console or API to created the endpoint configuration and deploy the stages.<p>\n",
    "\n",
    "Conceptually, the steps required for publishing are:<p>\n",
    "1. Make the`predictor.py` files executable\n",
    "2. Create an ECR repository within your default region\n",
    "3. Build a docker container with an identifieable name\n",
    "4. Tage the image and publish to the ECR repository\n",
    "<p><br>\n",
    "All of these are conveniently encapsulated inside `build_and_push` script. We simply run it with the unique name of our production run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_type='cpu'\n",
    "instance_class = \"p2\" if run_type.lower()=='gpu' else \"t2\"\n",
    "instance_type = \"ml.{}.xlarge\".format(instance_class)\n",
    "pipeline_name = 'gender-classifier'\n",
    "run='1'\n",
    "\n",
    "run_name = pipeline_name+\"-\"+run\n",
    "if run_type == \"cpu\":\n",
    "    !cp \"Dockerfile.cpu\" \"Dockerfile\"\n",
    "\n",
    "if run_type == \"gpu\":\n",
    "    !cp \"Dockerfile.gpu\" \"Dockerfile\"\n",
    "    \n",
    "!sh build_and_push.sh $run_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestration\n",
    "\n",
    "At this point, we can head to ECS console, grab the ARN for the repository where we published the docker image, and use SageMaker console to create hosted model, and endpoint.<p>\n",
    "However, it is often more convenient to automate these steps. In this notebook we do exactly that using `boto3 SageMaker` API.<p>\n",
    "Following are the steps:<p>\n",
    "    \n",
    "* First we create a model hosting definition, by providing the S3 location to the model artifact, and ARN to the ECR image of the container.\n",
    "* Using the model hosting definition, our next step is to create configuration of a hosted endpoint that will be used to serve prediciton generation requests. \n",
    "* Creating the endpoint is the last step in the ML cycle, that prepares your model to serve client reqests from applications.\n",
    "* We wait until the provision is completed and the endpoint in service. At this point we can send request to this endpoint and obtain gender predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sm_role = sagemaker.get_execution_role()\n",
    "print(\"Using Role {}\".format(sm_role))\n",
    "acc = boto3.client('sts').get_caller_identity().get('Account')\n",
    "reg = boto3.session.Session().region_name\n",
    "sagemaker = boto3.client('sagemaker')\n",
    "\n",
    "#Check if model already exists\n",
    "model_name = \"{}-model\".format(run_name)\n",
    "models = sagemaker.list_models(NameContains=model_name)['Models']\n",
    "model_exists = False\n",
    "if len(models) > 0:\n",
    "    for model in models:\n",
    "        if model['ModelName'] == model_name:\n",
    "            model_exists = True\n",
    "            break\n",
    "#Delete model, if chosen\n",
    "if model_exists == True:    \n",
    "    choice = input(\"Model already exists, do you want to delete and create a fresh one (Y/N) ? \")\n",
    "    if choice.upper()[0:1] == \"Y\":\n",
    "        sagemaker.delete_model(ModelName = model_name)\n",
    "        model_exists = False\n",
    "    else:\n",
    "        print(\"Model - {} already exists\".format(model_name))\n",
    "\n",
    "if model_exists == False:    \n",
    "    model_response = sagemaker.create_model(\n",
    "        ModelName=model_name,\n",
    "        PrimaryContainer={\n",
    "            'Image': '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(acc, reg, run_name),\n",
    "            'ModelDataUrl': 's3://{}/model/model.tar.gz'.format(s3bucketname)\n",
    "        },\n",
    "        ExecutionRoleArn=sm_role,\n",
    "        Tags=[\n",
    "            {\n",
    "                'Key': 'Name',\n",
    "                'Value': model_name\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(\"{} Created at {}\".format(model_response['ModelArn'], \n",
    "                                    model_response['ResponseMetadata']['HTTPHeaders']['date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if endpoint configuration already exists\n",
    "endpoint_config_name = \"{}-endpoint-config\".format(run_name)\n",
    "endpoint_configs = sagemaker.list_endpoint_configs(NameContains=endpoint_config_name)['EndpointConfigs']\n",
    "endpoint_config_exists = False\n",
    "if len(endpoint_configs) > 0:\n",
    "    for endpoint_config in endpoint_configs:\n",
    "        if endpoint_config['EndpointConfigName'] == endpoint_config_name:\n",
    "            endpoint_config_exists = True\n",
    "            break\n",
    "            \n",
    "#Delete endpoint configuration, if chosen\n",
    "if endpoint_config_exists == True:    \n",
    "    choice = input(\"Endpoint Configuration already exists, do you want to delete and create a fresh one (Y/N) ? \")\n",
    "    if choice.upper()[0:1] == \"Y\":\n",
    "        sagemaker.delete_endpoint_config(EndpointConfigName = endpoint_config_name)\n",
    "        endpoint_config_exists = False\n",
    "    else:\n",
    "        print(\"Endpoint Configuration - {} already exists\".format(endpoint_config_name))\n",
    "        \n",
    "if endpoint_config_exists == False:           \n",
    "    endpoint_config_response = sagemaker.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "            {\n",
    "                'VariantName': 'default',\n",
    "                'ModelName': model_name,\n",
    "                'InitialInstanceCount': 1,\n",
    "                'InstanceType': instance_type,\n",
    "                'InitialVariantWeight': 1\n",
    "            },\n",
    "        ],\n",
    "        Tags=[\n",
    "            {\n",
    "                'Key': 'Name',\n",
    "                'Value': endpoint_config_name\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    print(\"{} Created at {}\".format(endpoint_config_response['EndpointConfigArn'], \n",
    "                                    endpoint_config_response['ResponseMetadata']['HTTPHeaders']['date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "#Check if endpoint already exists\n",
    "endpoint_name = \"{}-endpoint\".format(run_name)\n",
    "endpoints = sagemaker.list_endpoints(NameContains=endpoint_name)['Endpoints']\n",
    "endpoint_exists = False\n",
    "if len(endpoints) > 0:\n",
    "    for endpoint in endpoints:\n",
    "        if endpoint['EndpointName'] == endpoint_name:\n",
    "            endpoint_exists = True\n",
    "            break\n",
    "            \n",
    "#Delete endpoint, if chosen\n",
    "if endpoint_exists == True:    \n",
    "    choice = input(\"Endpoint already exists, do you want to delete and create a fresh one (Y/N) ? \")\n",
    "    if choice.upper()[0:1] == \"Y\":\n",
    "        sagemaker.delete_endpoint(EndpointName = endpoint_name)\n",
    "        print(\"Deleting Endpoint - {} ...\".format(endpoint_name))\n",
    "        waiter = sagemaker.get_waiter('endpoint_deleted')\n",
    "        waiter.wait(EndpointName=endpoint_name,\n",
    "                   WaiterConfig = {'Delay':1,'MaxAttempts':100})\n",
    "        endpoint_exists = False\n",
    "        print(\"Endpoint - {} deleted\".format(endpoint_name))\n",
    "        \n",
    "    else:\n",
    "        print(\"Endpoint - {} already exists\".format(endpoint_name))\n",
    "        \n",
    "if endpoint_exists == False:  \n",
    "\n",
    "    endpoint_response = sagemaker.create_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        Tags=[\n",
    "            {\n",
    "                'Key': 'string',\n",
    "                'Value': endpoint_name\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    status='Creating'\n",
    "    sleep = 3\n",
    "\n",
    "    print(\"{} Endpoint : {}\".format(status,endpoint_name))\n",
    "    bar = widgets.FloatProgress(min=0, description=\"Progress\") # instantiate the bar\n",
    "    display(bar) # display the bar\n",
    "\n",
    "    while status != 'InService' and status != 'Failed' and status != 'OutOfService':    \n",
    "        endpoint_response = sagemaker.describe_endpoint(\n",
    "            EndpointName=endpoint_name\n",
    "        )\n",
    "        status = endpoint_response['EndpointStatus']\n",
    "        time.sleep(sleep)\n",
    "        bar.value = bar.value + 1 \n",
    "        if bar.value >= bar.max-1:\n",
    "            bar.max = int(bar.max*1.05)\n",
    "        if status != 'InService' and status != 'Failed' and status != 'OutOfService':        \n",
    "            print(\".\", end='')\n",
    "\n",
    "    bar.max = bar.value     \n",
    "    html = widgets.HTML(\n",
    "        value=\"<H2>Endpoint <b><u>{}</b></u> - {}</H2>\".format(endpoint_response['EndpointName'], status)\n",
    "    )\n",
    "    display(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_name = input(\"Enter name : \")\n",
    "val_no_special = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', test_name)\n",
    "val_space_collapse = re.sub( '\\s+', ' ', val_no_special).strip()\n",
    "names_test = word_tokenize(val_space_collapse)\n",
    "request_body = \",\".join(names_test)\n",
    "\n",
    "!aws sagemaker-runtime invoke-endpoint --endpoint-name \"$run_name-endpoint\" --body \"$request_body\" --content-type text/csv outfile\n",
    "!cat outfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
