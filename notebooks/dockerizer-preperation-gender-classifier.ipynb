{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Prediction from name, using Deep Learning\n",
    "\n",
    "Deep Neural Networks can be used to extract features in the input and derive higher level abstractions. This technique is used regularly in vision, speech and text analysis. In this exercise, we build a deep learning model that would identify low level features in texts containing people's names, and would be able to classify them in one of two categories - Male or Female.\n",
    "\n",
    "## Recurrent Neural Networks and Long Short Term Memory\n",
    "Since we have to process sequence of characters, Recurrent Neural Netwrosk are a good fit for this problem. Whenever we have to persist learning from data previously seen, traditional Neural Networks fail. Recurrent Neural Networks contains loops in the graph, that allows them to persist data in memory. Effective the loops facilitate passing multiple copies of information to be passed on to next step.\n",
    "<details>\n",
    "<summary><strong>Recurrent Neural Network - Loops (expand to view diagram)</strong></summary><p>\n",
    "    ![Recurrent Neural Network - Loops](images/RNN-unrolled.png \"Recurrent Neural Network - Loops\")\n",
    "</p></details>\n",
    "\n",
    "\n",
    "In practice however, when we need to selectively memorize or forget patterns seen in the past, based on the context, plain vanilla RNNs do not seem to perform so well. Instead we can use a special type of RNN, that can retain information in long term, and thus works better in understanding the contextual relation between patterns observed. They are known as Long Short Term memory.\n",
    "\n",
    "The nodes in an LSTM networks consusts of remember/forget gates to retain or pass patterns learnt in sequence useful for predicting target variable. These gates are a way to optionally let information through and tends to the ability of LSTM networks to remove or add information to the cell state in regulated manner.\n",
    "<details>\n",
    "<summary><strong>LSTM - Chains (expand to view diagram)</strong></summary><p>\n",
    "    ![LSTM - Chains](images/LSTM3-chain.png \"LSTM - Chains\")\n",
    "</p></details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture\n",
    "The problem we are trying to solve is to predict whether a given name belongs to a male or female. We will use supervised learning, where the character sequence making up the names would be `X` variable, and the flag indicating **Male(M)** or **Female(F)**  wuold be `Y` variable.\n",
    "\n",
    "We use a stacked 2-Layer LSTM model and a final dense layer with softmax activation as our network architecture. We use categorical cross-entropy as loss function, with an adam optimizer. We also add a 20% dropout layer is added for regularization to avoid over-fitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "*  We will use Keras deep learning library to build the network. THerefore we import the symbolic interfaces needed.\n",
    "* We also use pandas data frames to load and slice-and0dice data\n",
    "* Finally we need numpy for matric manipulation    \n",
    "* While running on SageMaker Notebook Instance, we choose conda_tensorflow kernel, so that Keras code is compiled to use tensorflow in the backend. \n",
    "* If you choose P2 and P3 class of instances for your Notebook, using Tensorflow ensures the low level code takes advantage of all available GPUs. So further dependencies needs to be installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "ebc23d43-698a-4a51-8bf6-791bc3d425c2"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import genfromtxt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.models import load_model\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download\n",
    "* Training data that we will be using to train the LSTM model is derived from US Government's SSA records of baby names registered. \n",
    "* Original dataset is split into separate text files for names registered every year, starting from 1880.\n",
    "Each record in each year's files contain the name, the gender identifier, and a count showing how many of those names have been registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir download ;  cd download ; wget https://www.ssa.gov/oact/babynames/names.zip ; unzip names.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step we concatenate data in all year specific files into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat download/yob* > download/allnames.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "As a first step, to facilitate convenient operation, we load the concatenated data as-is into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'download/allnames.txt'\n",
    "df=pd.read_csv(filename, sep=',', names = [\"Name\", \"Gender\", \"Count\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, when all files are concatenated, there will be multiple duplicate entries, because same name do get used year after year, in registration.<p>\n",
    "We test our assumption on duplicate entries, by taking any name, e.g. Mary, as example, and filtering all records containing that name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Name'] == 'Mary'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice here, that same name, `Mary` has been used both as Male and Female name. This might actually throw the model off, and affect it's accuracy.\n",
    "\n",
    "To remediate this scenario, notice that some name are more popular as Female names, and some are more opular as Male names.\n",
    "\n",
    "Run the same experiment as above with another name, such as `John`, and notice that occurence of this name in male population is more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Name'] == 'John'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also observe that even though some names are used both as Male and Female names, they are more commonly used for one gender than the other. For example, `Mary` is more common as male name, whereas `John` is more common as male name, as we saw above.<p>\n",
    "Since the model we'll be building needs to map each name to specifically one gender, without loss of generality, we can prepare our training data set to have a fixed marker - `M` or `F` on any particular name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleanup\n",
    "We'll remediate the solution using following approach:\n",
    "* Order the names by Name and Gender\n",
    "* Add the count for each group of unique Name-Gender combination\n",
    "* Iterate through the unique groups, and where a name is used for both Male and Female, choose to retain th entry with higher count\n",
    "* Create a new clean data frame containing only unique records mapping each name to a single gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = df.groupby( [ \"Name\", \"Gender\"] ).apply(lambda x: x.Count.sum()).to_frame()\n",
    "grouped_df.columns = ['Count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data is ordered by Name and Gender into a new frame, notice that the new frame contain the Name and Gender as index, and the total count of occurences as values.<p>\n",
    "We therefore create a dictionary that will have the Name as keys and gender (with higher sum count) as values.<p>\n",
    "We loop through the indexes of the grouped data frame and populate the entries into this dictionary following the logic as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names={}\n",
    "for i in range(len(grouped_df.index.values)):\n",
    "    #print(grouped_df.index[i][0] + \", \" + grouped_df.index[i][1] + \", \" + str(grouped_df.values[i][0]))\n",
    "    if i > 0 and grouped_df.index[i][0] == grouped_df.index[i-1][0]:\n",
    "        if grouped_df.values[i][0] > grouped_df.values[i-1][0]:\n",
    "            names[grouped_df.index[i][0]] = grouped_df.index[i][1]\n",
    "        else:\n",
    "            names[grouped_df.index[i][0]] = grouped_df.index[i-1][1]\n",
    "    else:\n",
    "        names[grouped_df.index[i][0]] = grouped_df.index[i][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dictionary is populated, we create a clean data frame using the keys and values as coulmns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = df = pd.DataFrame(list(names.items()), columns=['Name', 'Gender']).sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the cleaned up data only has unique records, and that it has single entries for the names - `Mary` and `John`, uniquely mapped to one gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_df.shape)\n",
    "print(clean_df.loc[clean_df['Name'] == 'Mary'])\n",
    "print(clean_df.loc[clean_df['Name'] == 'John'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we shuffle the data and save the clean data into a file, which we'll also use in subsequent phases of model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ../data\n",
    "clean_df.to_csv('../data/name-gender.txt',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "As you'll see in the notebook where we orchestrate a pipeline to train, deploy and host the model, the container you create will need access to data on an S3 bucket.<p>\n",
    "In order to prepare for the next step therefore, we'll do some pre-work here and upload the cleaned data to the S3 bucket that you created in module-1 of the workshop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we use boto3 CloudFormation API to get the output from the CloudFormation stack you created in module-1. The name of the hosting bucket you chose is available as an output fromt that stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "cfn = boto3.client('cloudformation')\n",
    "response = cfn.describe_stacks(\n",
    "    StackName='nlp-workshop-voc-webapp'\n",
    ")\n",
    "outputs = response['Stacks'][0]['Outputs']\n",
    "s3bucketname=\"\"\n",
    "for output in outputs:\n",
    "    if output['OutputKey'] == \"HostingBucket\":\n",
    "        s3bucketname = output['OutputValue']\n",
    "        break\n",
    "print(s3bucketname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you chose to manually create the VOC webapp, you can just specify the bucketname of your choosing and assign it to the `s3bucketname` variable.<p>\n",
    "In that case no need to query the CloudFormation stacks here.<p>\n",
    "Once we have our bucket name, we upload the data file under `/data/` prefix. This is the location we'll use during the final step, when we containerize and run the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file('../data/name-gender.txt', s3bucketname, 'data/name-gender.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can clean up some space by deleting the folder where we downloaded the unzipped the zip file from source.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature representation\n",
    "Before we start buiding the model, we need to represent the data in a format that we can feed into the LSTM model that we'll be creating.<p>\n",
    "Although we already have the cleaned data loaded as a data frame, let's load the data fresh from the S3 location. That way we'll know for sure that our cleaned data is of good quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f72fa157-ccea-4320-8b31-82e7a03a5f1d"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"https://s3.amazonaws.com/{}/data/name-gender.txt\".format(s3bucketname)\n",
    "data=pd.read_csv(filename, sep=',', names = [\"Name\", \"Gender\"])\n",
    "data = shuffle(data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick check on the record, and vaildate that we have the same number of records as we saved into the file after cleaning.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7f8ad84a-6f22-4c8f-8d1d-f4a41192892e"
    }
   },
   "outputs": [],
   "source": [
    "#number of names\n",
    "num_names = data.shape[0]\n",
    "print(num_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We need to convert the names into numeric arrays, usingone-hot encoding scheme. \n",
    "The length of the arrays representing the names need to be as long as the longest name record we have.\n",
    "Therefore we check for the longest name length and have it in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of longest name\n",
    "max_name_length = (data['Name'].map(len).max())\n",
    "print(max_name_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step of feature engineering we extract all names as an array, and derive the set of alphabets used in the names.<p>\n",
    "The way we choose to do so, is to concatenate all characters into one string, and then serive a `set`. By definition, a `set` in Python would contain only unique charatcers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "14cb65ea-da38-4469-908a-a21c49bbba16"
    }
   },
   "outputs": [],
   "source": [
    "names = data['Name'].values\n",
    "txt = \"\"\n",
    "for n in names:\n",
    "    txt += n.lower()\n",
    "print(len(txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we apply a `set` operation, we derive as many characters as there are alphabets in English language, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "efa1c69b-43a7-489e-90d0-c9b4b98e5888"
    }
   },
   "outputs": [],
   "source": [
    "chars = sorted(set(txt))\n",
    "alphabet_size = len(chars)\n",
    "print('Alphabet size:', len(chars))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for one-hot encoding to work, we nned to assign index values to each of these characters.<p>\n",
    "Since we have all alphabets `a` to `z`, the most natural index would be to just assign sequential values.<p>\n",
    "We create a Python `dictionary` with the character indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices = dict((str(chr(c)), i) for i, c in enumerate(range(97,123)))\n",
    "alphabet_size = 123-97\n",
    "for key in sorted(char_indices.keys()):\n",
    "    print(\"%s: %s\" % (key, char_indices[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we also need to somehow store the maximum length of a name record to be used later when we containerize our training and inference, as a good practice, let's also store that value as another entry into the same `dictionary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_indices['max_name_length'] = max_name_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoded array would be of dimension `n` `*` `m` `*` `a`, where :\n",
    "* `n` = Number of name records, \n",
    "* `m` = Maximum length of a record, and \n",
    "* `a` = Size of alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the `n` name records would be represented by 2-dimensional matrix of fixed size.<p>\n",
    "This matrix would have number of rows equal to the maximum length of a name record.<p>\n",
    "Each row would be of size equal to the alphabet size.<p>\n",
    "For each position of a character in a given name, a row of this 2-dimensinal matrix would be either all zeroes (if no alphabets present in the corresponding position), or a row vector with a `1` in the position of the alphabet indicated in the index (and zeroes in other positions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the name `Mary` would look like (note we ignore case by convertin names to lower case)<p>\n",
    "m => [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]<br>\n",
    "a => [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]<br>\n",
    "r => [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]<br>\n",
    "y => [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin the encoding by taking a tensor containing all zeroes. Observe the dimensions matches the above description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((num_names, max_name_length, alphabet_size))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we iterate through each character in each name records and selective turn the matching elements (as in the character index) to ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,name in enumerate(names):\n",
    "    name = name.lower()\n",
    "    for t, char in enumerate(name):\n",
    "        X[i, t,char_indices[char]] = 1\n",
    "X[0,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms do not work well when data has too much skewness.<p>\n",
    "So, let us validate tjhat both genders are somewhat equally represented in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2b85f255-be7a-4caa-b4f5-cf0eca3072cc"
    }
   },
   "outputs": [],
   "source": [
    "data['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `X` variables of training data one-hot encoded, it is time to encode the traget `Y` variable.<p>\n",
    "To do so, we simply create a column vector with zeroes representing Female and ones represnting Male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7a7851c4-dd03-42a2-90e7-edfaea3c87cd"
    }
   },
   "outputs": [],
   "source": [
    "Y = np.ones((num_names,2))\n",
    "Y[data['Gender'] == 'F',0] = 0\n",
    "Y[data['Gender'] == 'M',1] = 0\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last check to ensure that dimensions of `X` and `Y` are compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "6ba83249-f57a-458d-bd22-04891354cec5"
    }
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1add41ec-20c9-4e24-8b60-87b1034e6f4c"
    }
   },
   "outputs": [],
   "source": [
    "data_dim = alphabet_size\n",
    "timesteps = max_name_length\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "We build a stacked LSTM network with a final dense layer with softmax activation (many-to-one setup).<p>\n",
    "Categorical cross-entropy loss is used with adam optimizer.<p>\n",
    "A 20% dropout layer is added for regularization to avoid over-fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "34f06cf2-7beb-4fb8-9627-e9e029a17256"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(512, return_sequences=True, input_shape=(timesteps, data_dim)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(512, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model training\n",
    "We train this model for 10 epochs, with a batch size of 64. Too large a batch size may result in out of memory error.<p>\n",
    "During training we designate 20% of training data (randomly chosen) to be used as validation data. Validation is never presented to the model during training, instead used to ensure that the model works well with data that it has never seen.<p>\n",
    "This confirms we are not over-fitting, that is the model is not simply memoriziing the dat it sees, and that it can generalize it's learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "28301990-bd58-40da-8600-99511131e2b1"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X, Y, validation_split=0.20, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training for only 5 epochs, if everything goes well, you should see about 86% of accuracy, both over training and validation data, which is a pretty good result in itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model testing\n",
    "To test the accuracy of the model, I took a list of fourth grader students from my son's class in their school.<p>\n",
    "Same data formatting, as we did previously on training data (one-hot encoding using the same character indices)would be needed here as well.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "23fb2c07-a8ac-4675-95b2-8be5924ad8af"
    }
   },
   "outputs": [],
   "source": [
    "names_test = [\"Alyse\",\"Hannah\",\"Carter\",\"Soren\",\"Vihaan\",\"Samantha\",\"Drew\",\"Mica\",\"Talie\",\"Abhiram\",\"Zunairah\",\"Humairah\",\"Tate\",\"Dawson\",\"Finn\",\"Cavan\",\"Cade\",\"Karenna\",\"Emmett\",\"Zada\",\"Ethan\"]\n",
    "num_test = len(names_test)\n",
    "\n",
    "X_test = np.zeros((num_test, max_name_length, alphabet_size))\n",
    "\n",
    "for i,name in enumerate(names_test):\n",
    "    name = name.lower()\n",
    "    for t, char in enumerate(name):\n",
    "        X_test[i, t,char_indices[char]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed this one-hot encoded test data to the model, and the `predict` generates a vector, similar to the training labels vector we used before. Except in this case, it contains what model thinks the gender represnted by each of the test records.<p>\n",
    "To present data intutitively, we simply map it back to `Male` / `Female`, from the `0` / `1` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "for i,name in enumerate(names_test):\n",
    "    print(\"{} ({})\".format(names_test[i],\"M\" if predictions[i][0]>predictions[i][1] else \"F\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My son validated, that all the boys and girls had their genders correctly identified by our simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model saving\n",
    "Our job is done, we satisfied ourselves that the scheme works, and that we have a somewhat useful model that we can use to predict the gender of people from their names.<p>\n",
    "In order to orchestrate the ML pipeline however, we need to confirm that the model can be saved and loaded from disk, and still be able to generate same predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to save the model file (containing the weights), and the character indices (including the length of maximum name).<p>\n",
    "This is why we saved the maximum name length as another entry into the dictionary of characters, so that we can load both at the same time.<p>\n",
    "Note however that, using this scheme, our ability to generate prediction is limited to the name of length upto the maximum length of names among the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('GenderLSTM.h5')\n",
    "char_indices['max_name_length'] = max_name_length\n",
    "np.save('GenderLSTM.npy', char_indices) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently we load the saved model from the files on the disk, and check to see the indices are loaded, as saved.<p>\n",
    "We have no way to directly assert the equality of the model weights though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('GenderLSTM.h5')\n",
    "loaded_char_indices = np.load('GenderLSTM.npy').item()\n",
    "max_name_length = loaded_char_indices['max_name_length']\n",
    "loaded_char_indices.pop('max_name_length', None)\n",
    "alphabet_size = len(loaded_char_indices)\n",
    "print(loaded_char_indices)\n",
    "print(max_name_length)\n",
    "print(alphabet_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we run a similar test as we did with the freshly created model.<p>\n",
    "It should exhibit the same level of accuracy when presented with any previously unseen names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_test = [\"Shahrukh\", \"Rob\", \"Victor\",\"Amaya\",\"Vetri\", \"Swetha\",\"Binoy\",\"Moni\",\"Deep\",\"Deepa\",\"Rupu\",\"Rupa\",\"Gurpreet\", \"Kanadpriya\",\"Kanad\",\"Treena\",\"Dean\",\"Osei\",\"Rui\",]\n",
    "num_test = len(names_test)\n",
    "\n",
    "X_test = np.zeros((num_test, max_name_length, alphabet_size))\n",
    "\n",
    "for i,name in enumerate(names_test):\n",
    "    name = name.lower()\n",
    "    for t, char in enumerate(name):\n",
    "        X_test[i, t,loaded_char_indices[char]] = 1\n",
    "\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "for i,name in enumerate(names_test):\n",
    "    print(\"{} ({})\".format(names_test[i],\"M\" if predictions[i][0]>predictions[i][1] else \"F\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the next step, we'll use a separate notebook to containerize the training and prediction code, execute the training on SageMaker using appropriate container, and host the model behind an API endpoint.<p>\n",
    "This would allow us to use the model from web-application, and put it into real use from our VoC application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head back to Module-3 of the workshop now, to the section titled - `Containerization`, and follow the steps described."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
